{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import scipy\n",
    "import random\n",
    "import tensorflow as tf\n",
    "import time\n",
    "import os\n",
    "import gym\n",
    "from gym import envs, scoreboard\n",
    "from gym.spaces import Discrete, Box\n",
    "import tempfile\n",
    "import sys\n",
    "from PGActor import PGActor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "# ==========================\n",
    "#   Training Parameters\n",
    "# ==========================\n",
    "# Max training steps\n",
    "MAX_EPISODES = 10000\n",
    "# Max episode length\n",
    "MAX_EP_STEPS = 1000\n",
    "# Base learning rate for the Actor network\n",
    "ACTOR_LEARNING_RATE = 0.003\n",
    "# Discount factor \n",
    "GAMMA = 0.99\n",
    "\n",
    "# ===========================\n",
    "#   Utility Parameters\n",
    "# ===========================\n",
    "# Render gym env during training\n",
    "RENDER_ENV = True\n",
    "# Use Gym Monitor\n",
    "GYM_MONITOR_EN = True\n",
    "# Gym environment\n",
    "ENV_NAME = 'Pendulum-v0'\n",
    "# # Directory for storing gym results\n",
    "# MONITOR_DIR = './results/gym_ddpg'\n",
    "# # Directory for storing tensorboard summary results\n",
    "# SUMMARY_DIR = './results/tf_ddpg'\n",
    "RANDOM_SEED = 1337\n",
    "MINIBATCH_SIZE = 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def train(sess, env, actor):\n",
    "    \n",
    "    # Initialize our Tensorflow variables\n",
    "    sess.run(tf.initialize_all_variables())\n",
    "\n",
    "    # Initialize memory\n",
    "    old_states = []\n",
    "    old_actions = []\n",
    "    old_values = []\n",
    "#     old_probs = []\n",
    "    mem_fill = 0\n",
    "    \n",
    "    for i in range(MAX_EPISODES):\n",
    "        s = env.reset()\n",
    "#         print \"Episode\", i\n",
    "\n",
    "        ep_reward = 0\n",
    "        if mem_fill >= MINIBATCH_SIZE:\n",
    "            \n",
    "#             print \"Training on\", mem_fill, \"examples\"\n",
    "            actor.train(old_states, old_actions, old_values)\n",
    "#             print \"Trained!\"\n",
    "            old_states = []\n",
    "            old_actions = []\n",
    "            old_values = []\n",
    "#             old_probs = []\n",
    "            mem_fill = 0\n",
    "    \n",
    "        for j in range(MAX_EP_STEPS):\n",
    "            \n",
    "            if RENDER_ENV:\n",
    "                env.render()\n",
    "            \n",
    "            # generating a step\n",
    "            \n",
    "            # get the distribution parameters, sample and determine their probs\n",
    "            probs = actor.predict(np.reshape(s, (1, -1)))\n",
    "#             means, stds = wrapped_means[0], wrapped_stds[0]\n",
    "#             print \"means, stds =\",  (means, stds)\n",
    "#             a = map((lambda mean, std: np.random.normal(mean, std)),\n",
    "#                     means, stds)\n",
    "            \n",
    "            if np.random.uniform() < probs[0][0]:\n",
    "                a = np.asarray([0])\n",
    "            else:\n",
    "                a = np.asarray([1])\n",
    "#             vars = stds**2\n",
    "#             probs = np.exp(-((a-means)**2)/(2*vars))/np.sqrt(2* vars * np.pi)\n",
    "            \n",
    "            # get new state and reward\n",
    "            s2, r, is_done, info = env.step(a[0])\n",
    "            \n",
    "            # add step to batch\n",
    "            \n",
    "            old_states.append(np.reshape(s, (actor.s_dim,)))\n",
    "            old_actions.append(np.reshape(a, (actor.a_dim,)))\n",
    "#             old_probs.append(np.reshape(probs, (actor.a_dim,)))\n",
    "           \n",
    "            # keep adding steps until there are enough to do a training update\n",
    "            \n",
    "#             if replay_buffer.size() > MINIBATCH_SIZE:\n",
    "#                 s_batch, a_batch, r_batch, is_done_batch, s2_batch = \\\n",
    "#                     replay_buffer.sample_batch(MINIBATCH_SIZE)\n",
    "                \n",
    "                \n",
    "#                 # calculate targets\n",
    "#                 target_qs = critic.predict_target(s2_batch, actor.predict_target(s2_batch))\n",
    "                \n",
    "#                 #if the game has ended target_q not added to get hindsight q\n",
    "#                 hindsight_q_vec = (r_batch + (1 - is_done_batch.astype(float)) * GAMMA * np.reshape(target_qs, (MINIBATCH_SIZE,)))\n",
    "# #                 print \"r_batch has shape: \", r_batch.shape\n",
    "# #                 print \"is_done_batch has shape: \", is_done_batch.shape\n",
    "# #                 print \"target_qs has shape: \", target_qs.shape\n",
    "# #                 print \"H q vec has shape: \", hindsight_q_vec.shape\n",
    "                \n",
    "#                 #critic training\n",
    "                \n",
    "#                 predicted_q_value, _ = critic.train(s_batch, a_batch,\n",
    "#                                                      np.reshape(hindsight_q_vec, (MINIBATCH_SIZE, 1)))\n",
    "#                 ep_ave_max_q += np.amax(predicted_q_value)\n",
    "                \n",
    "#                 # actor training\n",
    "#                 actions = actor.predict(s_batch)\n",
    "#                 dQda_list = critic.action_gradients(s_batch, actions) # could repeat more than once, or even less than once\n",
    "#                 actor.train(s_batch, dQda_list[0])\n",
    "                \n",
    "            s=s2\n",
    "                \n",
    "            ep_reward += r\n",
    "                \n",
    "            if is_done or j == MAX_EP_STEPS-1:\n",
    "#                 if is_done:\n",
    "#                     print \"Done for some reason\"\n",
    "                ep_len = j+1\n",
    "                mem_fill += ep_len\n",
    "                values = ep_len * [np.asarray([ep_reward])]\n",
    "                old_values = old_values + values\n",
    "                if i % 100 ==0:\n",
    "                    print '| Reward: %.2i' % int(ep_reward), \" | Episode\", i\n",
    "                break\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2016-11-18 20:18:27,286] Making new env: CartPole-v0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| Reward: 10  | Episode 0\n",
      "| Reward: 08  | Episode 100\n",
      "| Reward: 10  | Episode 200\n",
      "| Reward: 10  | Episode 300\n",
      "| Reward: 10  | Episode 400\n",
      "| Reward: 08  | Episode 500\n",
      "| Reward: 09  | Episode 600\n",
      "| Reward: 10  | Episode 700\n",
      "| Reward: 09  | Episode 800\n",
      "| Reward: 09  | Episode 900\n",
      "| Reward: 10  | Episode 1000\n",
      "| Reward: 10  | Episode 1100\n",
      "| Reward: 08  | Episode 1200\n",
      "| Reward: 08  | Episode 1300\n",
      "| Reward: 08  | Episode 1400\n",
      "| Reward: 10  | Episode 1500\n",
      "| Reward: 08  | Episode 1600\n",
      "| Reward: 10  | Episode 1700\n",
      "| Reward: 08  | Episode 1800\n",
      "| Reward: 09  | Episode 1900\n",
      "| Reward: 09  | Episode 2000\n",
      "| Reward: 09  | Episode 2100\n",
      "| Reward: 10  | Episode 2200\n",
      "| Reward: 09  | Episode 2300\n",
      "| Reward: 11  | Episode 2400\n",
      "| Reward: 09  | Episode 2500\n",
      "| Reward: 10  | Episode 2600\n",
      "| Reward: 10  | Episode 2700\n",
      "| Reward: 10  | Episode 2800\n",
      "| Reward: 10  | Episode 2900\n",
      "| Reward: 08  | Episode 3000\n",
      "| Reward: 10  | Episode 3100\n",
      "| Reward: 09  | Episode 3200\n",
      "| Reward: 11  | Episode 3300\n",
      "| Reward: 11  | Episode 3400\n",
      "| Reward: 10  | Episode 3500\n",
      "| Reward: 10  | Episode 3600\n",
      "| Reward: 09  | Episode 3700\n",
      "| Reward: 09  | Episode 3800\n",
      "| Reward: 10  | Episode 3900\n",
      "| Reward: 09  | Episode 4000\n",
      "| Reward: 10  | Episode 4100\n",
      "| Reward: 11  | Episode 4200\n",
      "| Reward: 09  | Episode 4300\n",
      "| Reward: 09  | Episode 4400\n",
      "| Reward: 10  | Episode 4500\n",
      "| Reward: 09  | Episode 4600\n",
      "| Reward: 08  | Episode 4700\n",
      "| Reward: 08  | Episode 4800\n",
      "| Reward: 10  | Episode 4900\n",
      "| Reward: 10  | Episode 5000\n",
      "| Reward: 08  | Episode 5100\n",
      "| Reward: 08  | Episode 5200\n",
      "| Reward: 09  | Episode 5300\n",
      "| Reward: 09  | Episode 5400\n",
      "| Reward: 09  | Episode 5500\n",
      "| Reward: 08  | Episode 5600\n",
      "| Reward: 09  | Episode 5700\n",
      "| Reward: 09  | Episode 5800\n",
      "| Reward: 10  | Episode 5900\n",
      "| Reward: 09  | Episode 6000\n",
      "| Reward: 09  | Episode 6100\n",
      "| Reward: 09  | Episode 6200\n",
      "| Reward: 09  | Episode 6300\n",
      "| Reward: 10  | Episode 6400\n",
      "| Reward: 10  | Episode 6500\n",
      "| Reward: 09  | Episode 6600\n",
      "| Reward: 10  | Episode 6700\n",
      "| Reward: 10  | Episode 6800\n",
      "| Reward: 09  | Episode 6900\n",
      "| Reward: 11  | Episode 7000\n",
      "| Reward: 08  | Episode 7100\n",
      "| Reward: 08  | Episode 7200\n",
      "| Reward: 09  | Episode 7300\n",
      "| Reward: 10  | Episode 7400\n",
      "| Reward: 09  | Episode 7500\n",
      "| Reward: 10  | Episode 7600\n",
      "| Reward: 10  | Episode 7700\n",
      "| Reward: 09  | Episode 7800\n",
      "| Reward: 09  | Episode 7900\n",
      "| Reward: 11  | Episode 8000\n",
      "| Reward: 09  | Episode 8100\n",
      "| Reward: 09  | Episode 8200\n",
      "| Reward: 09  | Episode 8300\n",
      "| Reward: 10  | Episode 8400\n",
      "| Reward: 09  | Episode 8500\n",
      "| Reward: 10  | Episode 8600\n",
      "| Reward: 09  | Episode 8700\n",
      "| Reward: 08  | Episode 8800\n",
      "| Reward: 08  | Episode 8900\n",
      "| Reward: 09  | Episode 9000\n",
      "| Reward: 09  | Episode 9100\n",
      "| Reward: 09  | Episode 9200\n",
      "| Reward: 08  | Episode 9300\n",
      "| Reward: 10  | Episode 9400\n",
      "| Reward: 08  | Episode 9500\n",
      "| Reward: 09  | Episode 9600\n",
      "| Reward: 09  | Episode 9700\n",
      "| Reward: 09  | Episode 9800\n",
      "| Reward: 10  | Episode 9900\n"
     ]
    }
   ],
   "source": [
    "# defining environment\n",
    "sess = tf.Session()\n",
    "\n",
    "env = gym.make('CartPole-v0')\n",
    "# print env.observation_space\n",
    "# state_dim = env.observation_space.shape[0]\n",
    "# action_dim = env.action_space.shape[0]\n",
    "# print \"state and actions dims:\", state_dim, action_dim\n",
    "\n",
    "# make sure action bound is symmetric (can change in future,\n",
    "# but need to remember to scale actor output appropriately)\n",
    "# assert (env.action_space.high == -env.action_space.low)\n",
    "\n",
    "# action_bound = env.action_space.high\n",
    "\n",
    "# start up actor and critic pair\n",
    "\n",
    "actor = PGActor(sess, 4, 1, 1, \n",
    "                 ACTOR_LEARNING_RATE)\n",
    "\n",
    "train(sess, env, actor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
