{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import scipy\n",
    "import random\n",
    "import tensorflow as tf\n",
    "import time\n",
    "import os\n",
    "import gym\n",
    "from gym import envs, scoreboard\n",
    "from gym.spaces import Discrete, Box\n",
    "import tempfile\n",
    "import sys\n",
    "from PGActorDiscrete import PGActorDiscrete"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "# ==========================\n",
    "#   Training Parameters\n",
    "# ==========================\n",
    "# Max training steps\n",
    "MAX_EPISODES = 10000\n",
    "# Max episode length\n",
    "MAX_EP_STEPS = 1000\n",
    "# Base learning rate for the Actor network\n",
    "ACTOR_LEARNING_RATE = 0.03\n",
    "CRITIC_LEARNING_RATE = 0.1\n",
    "# Discount factor \n",
    "GAMMA = 0.99\n",
    "\n",
    "# ===========================\n",
    "#   Utility Parameters\n",
    "# ===========================\n",
    "# Render gym env during training\n",
    "RENDER_ENV = True\n",
    "# Use Gym Monitor\n",
    "GYM_MONITOR_EN = True\n",
    "# Gym environment\n",
    "ENV_NAME = 'Pendulum-v0'\n",
    "# # Directory for storing gym results\n",
    "# MONITOR_DIR = './results/gym_ddpg'\n",
    "# # Directory for storing tensorboard summary results\n",
    "# SUMMARY_DIR = './results/tf_ddpg'\n",
    "RANDOM_SEED = 1337\n",
    "MINIBATCH_SIZE = 200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def train(sess, env, actor):\n",
    "    \n",
    "    # Initialize our Tensorflow variables\n",
    "    sess.run(tf.initialize_all_variables())\n",
    "\n",
    "    # Initialize memory\n",
    "    old_states = []\n",
    "    old_actions = []\n",
    "    old_advantages = []\n",
    "    old_hindsight_values = []\n",
    "    mem_fill = 0\n",
    "    nep_reward = 0\n",
    "    nep_est_reward = 0\n",
    "    \n",
    "    for i in range(MAX_EPISODES):\n",
    "        \n",
    "        s = env.reset()\n",
    "        v = actor.predict_value(np.reshape(s, (1, -1)))[0]\n",
    "        nep_est_reward += v\n",
    "        ep_reward = 0\n",
    "        advantages = []\n",
    "        \n",
    "        for j in range(MAX_EP_STEPS):\n",
    "            \n",
    "            if RENDER_ENV:\n",
    "                env.render()\n",
    "            \n",
    "            # get the probabilities and sample for an action\n",
    "            probs = actor.predict(np.reshape(s, (1, -1)))\n",
    "            a = [np.random.choice(range(actor.a_dim+1), p=probs[0])]\n",
    "                                  \n",
    "            # get new state and reward\n",
    "            s2, r, is_done, info = env.step(a[0])\n",
    "            \n",
    "            # get new value prediction and find delta\n",
    "            v2 = actor.predict_value(np.reshape(s2, (1, -1)))[0]\n",
    "            \n",
    "            if is_done:\n",
    "                hs_v = [r]\n",
    "            else:\n",
    "                hs_v = r + GAMMA * v2\n",
    "                \n",
    "            # add step to batch\n",
    "            old_states.append(np.reshape(s, (actor.s_dim,)))\n",
    "            old_actions.append(np.reshape(a, (actor.a_dim,)))\n",
    "            old_hindsight_values.append(hs_v)\n",
    "            advantages.append(-v) # this will get modified in future timesteps\n",
    "            for k in range(len(advantages)):\n",
    "                assert k <= j\n",
    "                advantages[k] += r * (GAMMA**(j-k))\n",
    "            \n",
    "            s=s2\n",
    "            v=v2\n",
    "            ep_reward += r\n",
    "            \n",
    "            # end of episode methods\n",
    "            if is_done or j == MAX_EP_STEPS-1: \n",
    "                ep_len = j+1\n",
    "                mem_fill += ep_len\n",
    "                old_advantages = old_advantages + advantages\n",
    "                nep_reward += ep_reward\n",
    "                break\n",
    "                                  \n",
    "        if i % 100 ==0 and i!=0:\n",
    "                print '| Avg value (100 eps): %.2i' % (int(nep_reward/100)), \\\n",
    "                \" | Avg est value (100 eps): \", nep_est_reward[0]/100, \" | Episode\", i\n",
    "                print\n",
    "                nep_reward=0\n",
    "                nep_est_reward = 0\n",
    "        if mem_fill >= MINIBATCH_SIZE:\n",
    "            \n",
    "#             print \"Training on\", mem_fill, \"examples\"\n",
    "#             print np.asarray(old_hindsight_values).shape\n",
    "            actor.train(old_states, old_actions, old_advantages)\n",
    "            actor.train_value(old_states, old_hindsight_values)\n",
    "            old_states = []\n",
    "            old_actions = []\n",
    "            old_advantages = []\n",
    "            old_hindsight_values = []\n",
    "                                \n",
    "            mem_fill = 0\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2016-11-22 18:42:17,021] Making new env: CartPole-v0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| Avg value (100 eps): 21  | Avg est value (100 eps):  11.7508044434  | Episode 100\n",
      "\n",
      "| Avg value (100 eps): 38  | Avg est value (100 eps):  25.2199462891  | Episode 200\n",
      "\n",
      "| Avg value (100 eps): 133  | Avg est value (100 eps):  50.9493505859  | Episode 300\n",
      "\n",
      "| Avg value (100 eps): 133  | Avg est value (100 eps):  55.2392285156  | Episode 400\n",
      "\n",
      "| Avg value (100 eps): 182  | Avg est value (100 eps):  77.3533935547  | Episode 500\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# defining environment\n",
    "sess = tf.Session()\n",
    "\n",
    "env = gym.make('CartPole-v0')\n",
    "# print env.observation_space\n",
    "# state_dim = env.observation_space.shape[0]\n",
    "# action_dim = env.action_space\n",
    "# print action_dim\n",
    "# print \"state and actions dims:\", state_dim, action_dim\n",
    "\n",
    "# make sure action bound is symmetric (can change in future,\n",
    "# but need to remember to scale actor output appropriately)\n",
    "# assert (env.action_space.high == -env.action_space.low)\n",
    "\n",
    "# action_bound = env.action_space.high\n",
    "\n",
    "# start up actor and critic pair\n",
    "\n",
    "actor = PGActorDiscrete(sess, 4, 1, \n",
    "                 ACTOR_LEARNING_RATE, CRITIC_LEARNING_RATE)\n",
    "\n",
    "train(sess, env, actor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
