{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import scipy.stats\n",
    "import random\n",
    "import tensorflow as tf\n",
    "import time\n",
    "import os\n",
    "import gym\n",
    "from gym import envs, scoreboard\n",
    "from gym.spaces import Discrete, Box\n",
    "import tempfile\n",
    "import sys\n",
    "from PGActorContinuous import PGActorContinuous\n",
    "%matplotlib notebook\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython import display"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# ==========================\n",
    "#   Training Parameters\n",
    "# ==========================\n",
    "# Max training steps\n",
    "MAX_EPISODES = 5000\n",
    "# Max episode length\n",
    "MAX_EP_STEPS = 1000\n",
    "# Base learning rate for the Actor network\n",
    "ACTOR_LEARNING_RATE = 0.01\n",
    "CRITIC_LEARNING_RATE = 0.03\n",
    "# Discount factor \n",
    "GAMMA = 0.99\n",
    "\n",
    "# ===========================\n",
    "#   Utility Parameters\n",
    "# ===========================\n",
    "# Render gym env during training\n",
    "RENDER_ENV = True\n",
    "# Use Gym Monitor\n",
    "GYM_MONITOR_EN = True\n",
    "# Gym environment\n",
    "ENV_NAME = 'Pendulum-v0'\n",
    "# # Directory for storing gym results\n",
    "# MONITOR_DIR = './results/gym_ddpg'\n",
    "# # Directory for storing tensorboard summary results\n",
    "# SUMMARY_DIR = './results/tf_ddpg'\n",
    "RANDOM_SEED = 1337\n",
    "EPS_PER_BATCH = 10\n",
    "# r = scipy.stats.truncnorm.rvs(a=-2.-mean, b=2.-mean, loc = mean, size=100000)\n",
    "# plt.hist(np.concatenate([r, [0]], 0))[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "IndentationError",
     "evalue": "unexpected indent (<ipython-input-3-37c5752d304a>, line 28)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m\"<ipython-input-3-37c5752d304a>\"\u001b[1;36m, line \u001b[1;32m28\u001b[0m\n\u001b[1;33m    means_wrapped, stds = actor.predict(np.reshape(s, (1, -1)))\u001b[0m\n\u001b[1;37m    ^\u001b[0m\n\u001b[1;31mIndentationError\u001b[0m\u001b[1;31m:\u001b[0m unexpected indent\n"
     ]
    }
   ],
   "source": [
    "ep_vals = []\n",
    "def train(sess, env, actor):\n",
    "    \n",
    "    \n",
    "\n",
    "    # Initialize memory\n",
    "    old_states = []\n",
    "    old_actions = []\n",
    "    old_advantages = []\n",
    "    old_returns = []\n",
    "    mem_fill = 0\n",
    "    nep_reward = 0\n",
    "    nep_est_reward = 0\n",
    "    for i in range(MAX_EPISODES):\n",
    "        s = env.reset()\n",
    "        v = actor.predict_value(np.reshape(s, (1, -1)))[0]\n",
    "        nep_est_reward += v\n",
    "        ep_reward = 0\n",
    "        returns = []\n",
    "        values = []\n",
    "        \n",
    "        for j in range(MAX_EP_STEPS):\n",
    "#             if i % EPS_PER_BATCH ==0:\n",
    "#                 time.sleep(0.02)\n",
    "#                 if RENDER_ENV:\n",
    "#                     env.render()\n",
    "            # get the action distribution parameters\n",
    "            means_wrapped, stds = actor.predict(np.reshape(s, (1, -1)))\n",
    "            means = means_wrapped[0]\n",
    "            try:\n",
    "                a = map((lambda mean, std : scipy.stats.truncnorm.rvs(a=(-actor.action_bound[0]-mean)/std,\n",
    "                                                                  b=(actor.action_bound[0]-mean)/std,\n",
    "                                                                  loc=mean,\n",
    "                                                                  scale=std)),\n",
    "                    means, stds)\n",
    "            except:\n",
    "                print stds\n",
    "            a = np.asarray(a) \n",
    "#             print a\n",
    "            if not (np.abs(a) <=2.).all():\n",
    "                print \"!!!Action outside bounds!!!!\"\n",
    "                print \"a =\",a\n",
    "                print \"means =\",means\n",
    "                print \"stds =\",stds\n",
    "            # get new state and reward\n",
    "            s2, r, is_done, info = env.step(a)\n",
    "            \n",
    "            # get new value prediction and find delta\n",
    "            v2 = actor.predict_value(np.reshape(s2, (1, -1)))[0]\n",
    "        \n",
    "            # add step to batch\n",
    "            old_states.append(np.reshape(s, (actor.s_dim,)))\n",
    "            old_actions.append(np.reshape(a, (actor.a_dim,)))\n",
    "            values.append(v)\n",
    "            returns.append([0])\n",
    "            for k in range(len(returns)):\n",
    "                assert k <= j\n",
    "                returns[k] += r * (GAMMA**(j-k))\n",
    "            s=s2\n",
    "            v = v2\n",
    "            ep_reward += r\n",
    "            \n",
    "            \n",
    "            if is_done or j == MAX_EP_STEPS-1:\n",
    "#                 if is_done:\n",
    "#                     print \"Done for some reason\"\n",
    "                ep_len = j+1\n",
    "                mem_fill += ep_len\n",
    "                \n",
    "                advantages = -np.asarray(values) + np.asarray(returns)\n",
    "#                 print np.asarray(values).shape\n",
    "#                 print np.asarray(returns).shape\n",
    "#                 print advantages.shape\n",
    "                old_advantages = old_advantages + advantages.tolist()\n",
    "                old_returns = old_returns + returns\n",
    "                nep_reward += ep_reward\n",
    "                break\n",
    "        ep_vals.append(ep_reward)\n",
    "        \n",
    "        if i % EPS_PER_BATCH ==0 and i!=0:\n",
    "                print \"| Avg value (\",EPS_PER_BATCH,\"eps):\",  (int(nep_reward/EPS_PER_BATCH)), \\\n",
    "                \" | Avg est value (\",EPS_PER_BATCH,\"eps):\", nep_est_reward[0]/EPS_PER_BATCH, \" | Episode\", i\n",
    "                print\n",
    "                nep_reward=0\n",
    "                nep_est_reward = 0\n",
    "                \n",
    "        if i% EPS_PER_BATCH==0 and i!=0:\n",
    "            print \"Training on\", mem_fill, \"examples\"\n",
    "#             print map(lambda x: x[0], old_advantages[:100])\n",
    "#             print np.asarray(old_states ).shape\n",
    "#             print np.asarray(old_actions ).shape\n",
    "            print np.asarray(old_advantages ).shape\n",
    "            actor.train(old_states, old_actions, old_advantages)\n",
    "            for i in range(50):    \n",
    "                actor.train_value(old_states, old_returns)\n",
    "            old_states =[]\n",
    "            old_actions = []\n",
    "            old_advantages = []   \n",
    "            old_returns = []\n",
    "            mem_fill = 0\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# defining environment\n",
    "sess = tf.Session()\n",
    "\n",
    "env = gym.make('Pendulum-v0')\n",
    "state_dim = env.observation_space.shape[0]\n",
    "action_dim = env.action_space.shape[0]\n",
    "print \"state and actions dims:\", state_dim, action_dim\n",
    "\n",
    "# make sure action bound is symmetric (can change in future,\n",
    "# but need to remember to scale actor output appropriately)\n",
    "assert (env.action_space.high == -env.action_space.low)\n",
    "\n",
    "action_bound = env.action_space.high\n",
    "print \"Action bound:\", action_bound\n",
    "actor = PGActorContinuous(sess,state_dim, action_dim, action_bound, \n",
    "                 ACTOR_LEARNING_RATE, CRITIC_LEARNING_RATE)\n",
    "# Initialize our Tensorflow variables\n",
    "sess.run(tf.initialize_all_variables())\n",
    "train(sess, env, actor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sess = tf.Session()\n",
    "train(sess, env, actor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
